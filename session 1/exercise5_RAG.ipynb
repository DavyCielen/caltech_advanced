{
  "nbformat": 4,
  "nbformat_minor": 4,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Exercise Notebook\n",
        "\n",
        "In this notebook, you’ll learn how to set up a **Retrieval-Augmented Generation (RAG)** pipeline using **LangChain**. RAG is a technique where we pair a Large Language Model (LLM) with a retrieval mechanism to ground the LLM’s responses with relevant data.\n",
        "\n",
        "Think of it like having a chatty assistant that can look up real information from documents before it answers. This way, your AI doesn’t just guess—it provides answers based on actual knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Required Libraries\n",
        "\n",
        "Before we begin, let’s ensure our environment has all the libraries we need. We’re using:\n",
        "- **langchain** for orchestrating our LLM and retrieval,\n",
        "- **openai** for the LLM (you can substitute another if you prefer),\n",
        "- **chromadb** as a vector store (where we’ll keep our document embeddings),\n",
        "- **tiktoken** for text tokenization.\n",
        "\n",
        "If any of these are missing in your environment, run the following cell to install them. If they’re already installed, you’ll just see a message saying they’re satisfied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 1: Install Required Libraries\n",
        "!pip install langchain openai chromadb tiktoken"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /Users/Davy/Library/Caches/pypoetry/virtualenvs/davy-PIAAdT6P-py3.11/lib/python3.11/site-packages (0.3.17)\n",
            "Requirement already satisfied: openai in /Users/Davy/Library/Caches/pypoetry/virtualenvs/davy-PIAAdT6P-py3.11/lib/python3.11/site-packages (1.60.2)\n",
            "Requirement already satisfied: chromadb in /Users/Davy/Library/Caches/pypoetry/virtualenvs/davy-PIAAdT6P-py3.11/lib/python3.11/site-packages (0.6.3)\n",
            "Requirement already satisfied: tiktoken in /Users/Davy/Library/Caches/pypoetry/virtualenvs/davy-PIAAdT6P-py3.11/lib/python3.11/site-packages (0.8.0)\n",
            "... (Truncated for brevity) ..."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Import Necessary Libraries\n",
        "\n",
        "In this section, we pull in the essential classes and functions we’ll use:\n",
        "\n",
        "- **TextLoader**: Loads text files for embedding,\n",
        "- **OpenAIEmbeddings**: Creates vector embeddings from text using OpenAI’s model,\n",
        "- **Chroma**: A vector store implementation that uses ChromaDB,\n",
        "- **OpenAI**: Our Language Model (though you can swap this out for others),\n",
        "- **RetrievalQA**: A LangChain chain that uses a retriever to find relevant chunks before passing them to the LLM for generation,\n",
        "- **RecursiveCharacterTextSplitter**: Splits larger text files into manageable chunks,\n",
        "- **os**: Lets us interface with our operating system (e.g., to handle paths, environment variables)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load and Prepare Documents\n",
        "\n",
        "Here, we’ll load the text we want our model to reference. We use a **TextLoader** to read data from a file—in our example, `caltech_advanced/session 1/sample_data.txt`.\n",
        "\n",
        "You’ll also see a **RecursiveCharacterTextSplitter**. Large documents need to be broken down into smaller, more digestible chunks, so our language model can handle them effectively. The `chunk_size` parameter decides how big each text piece is, and `chunk_overlap` ensures a little overlap so no important context gets cut off.\n",
        "\n",
        "After splitting, we store the resulting chunks in `documents` for the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# show files in current directory (so you can confirm the path/file exists)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<your api key>\"  # Replace with your actual OpenAI API key\n",
        "\n",
        "loader = TextLoader(\"caltech_advanced/session 1/sample_data.txt\")  # Make sure this file exists in the path\n",
        "raw_documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "documents = text_splitter.split_documents(raw_documents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "caltech_advanced          session 1          sample_data.txt\n",
            "chroma_db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Embedding and Vector Storage\n",
        "\n",
        "Next, we’ll create embeddings for our chunked documents and store them in a vector database. **OpenAIEmbeddings** turns each text chunk into a numerical vector. Then, we use **Chroma** to index and store those vectors.\n",
        "\n",
        "This step is crucial: it’s how the system can later find which document chunk is most relevant when you ask a question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(documents, embeddings)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Set Up the RAG Pipeline\n",
        "\n",
        "Here’s where we combine it all:\n",
        "1. Turn our `vectorstore` into a **retriever** with `as_retriever()`. This makes it easy to query.\n",
        "2. Create a **RetrievalQA** chain that uses our LLM (OpenAI) plus our new retriever.\n",
        "3. We set `return_source_documents=True` to see which documents were used for the answer.\n",
        "\n",
        "All of this ensures that when we ask a question, the chain retrieves the relevant chunks from `Chroma` before prompting the LLM to generate a final response.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "retriever = vectorstore.as_retriever()\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=OpenAI(),\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/n9/06yvy3495fd1m9twr1tb0n100000gp/T/ipykernel_93371/4185712420.py:3: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
            "  llm=OpenAI(),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Run a Sample Query\n",
        "\n",
        "Now it’s time to see our pipeline in action! We’ll ask a simple question—`What is the main topic of the document?`—and the chain will:\n",
        "\n",
        "1. Convert the question into a vector,\n",
        "2. Find the most relevant document chunk(s) in our `Chroma` store,\n",
        "3. Combine that info and pass it to the LLM for the final answer.\n",
        "\n",
        "Check the printed result to see if it captures the essence of your source text. If it’s accurate, then your RAG setup is working!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "query = \"What is the main topic of the document?\"\n",
        "result = rag_chain.invoke({\"query\": query})\n",
        "print(\"\\nAnswer:\", result[\"result\"])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Answer:  The main topic of the document is Artificial Intelligence (AI) and its various applications, including Natural Language Processing (NLP) and Machine Learning (ML). It also mentions potential ethical concerns surrounding AI development.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise for Students\n",
        "\n",
        "Here are some fun (and educational) tasks to deepen your understanding:\n",
        "\n",
        "1. **Ask Different Questions:** Modify the `query` variable and see how the answer changes.\n",
        "2. **Load a Different Document:** Try another text or PDF. Compare how the chain responds.\n",
   
      ]
    }
  ]
}
