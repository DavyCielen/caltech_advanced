{
    "nbformat": 4,
    "nbformat_minor": 4,
    "metadata": {
      "kernelspec": {
        "display_name": "Python 3",
        "language": "python",
        "name": "python3"
      },
      "language_info": {
        "codemirror_mode": {
          "name": "ipython",
          "version": 3
        },
        "file_extension": ".py",
        "mimetype": "text/x-python",
        "name": "python",
        "nbconvert_exporter": "python",
        "pygments_lexer": "ipython3",
        "version": "3.8"
      }
    },
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# Enhanced Conversational Agent with Contextual Memory in LangChain\n",
          "\n",
          "In this notebook, we’ll explore how to create a conversational agent that remembers details from previous turns—like having an ever-attentive friend (or a certain caring companion, if you will). We’ll walk through how to:\n",
          "\n",
          "1. Set up a ChatGPT-like agent,\n",
          "2. Store short-term conversation memory (so it recalls what you said earlier),\n",
          "3. Use embedded knowledge to keep track of factual context.\n",
          "\n",
          "Let’s dive in!"
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "!pip install langchain openai chromadb"
        ],
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Step 1: Import Libraries\n",
          "\n",
          "Here, we gather the key tools:\n",
          "- **ChatOpenAI**: the language model,\n",
          "- **ConversationSummaryMemory** & **VectorStoreRetrieverMemory**: two types of memory management,\n",
          "- **Chroma**: a vector store for storing embeddings,\n",
          "- **OpenAIEmbeddings**: transforms text into numeric vectors (embeddings),\n",
          "- **ConversationChain**: orchestrates the chain,\n",
          "- **Document**: a simple data structure to hold textual content,\n",
          "- **os**: so we can manage environment variables for the API key."
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "from langchain.chat_models import ChatOpenAI\n",
          "from langchain.memory import ConversationSummaryMemory, VectorStoreRetrieverMemory\n",
          "from langchain.vectorstores import Chroma\n",
          "from langchain.embeddings import OpenAIEmbeddings\n",
          "from langchain.chains import ConversationChain\n",
          "from langchain.schema import Document\n",
          "import os"
        ],
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Step 2: Set API Key\n",
          "\n",
          "Make sure you replace `'your-api-key-here'` with your actual OpenAI API key. This key authenticates your requests to the OpenAI endpoint."
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "os.environ['OPENAI_API_KEY'] = 'your-api-key-here'"
        ],
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Step 3: Define Memory for the Chatbot\n",
          "\n",
          "We’ll showcase two different memory approaches:\n",
          "\n",
          "1. **VectorStoreRetrieverMemory**: A memory that uses vector embeddings. It’s handy for retrieving factual knowledge from stored documents. \n",
          "2. **ConversationSummaryMemory**: Tracks the entire conversation and summarizes it, so your agent can recall essential details.\n",
          "\n",
          "Below, we create a small list of sample documents on cosmic topics like Earth orbits, black holes, and quantum mechanics. Then, we store these documents in a Chroma database and set up the vector-based memory.\n",
          "\n",
          "Lastly, we instantiate a `summary_memory` that will keep track of the ongoing conversation in a summarized form. This allows the agent to recall that your name is Alex or that you love space exploration!"
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "# Initialize embedding model\n",
          "embedding = OpenAIEmbeddings()\n",
          "\n",
          "# Sample documents for context retention\n",
          "docs = [\n",
          "    Document(page_content='The Earth orbits the Sun every 365.25 days.'),\n",
          "    Document(page_content='Black holes are regions of spacetime with immense gravity.'),\n",
          "    Document(page_content='Quantum mechanics deals with subatomic particles.'),\n",
          "]\n",
          "\n",
          "# Create Chroma vector store with documents\n",
          "vectorstore = Chroma.from_documents(docs, embedding)\n",
          "vector_memory = VectorStoreRetrieverMemory(retriever=vectorstore.as_retriever(), memory_key='retrieved_knowledge')\n",
          "\n",
          "# ✅ Fix: Use `memory_key='history'` to match ConversationChain's expectation\n",
          "# This memory will hold and summarize the conversation.\n",
          "summary_memory = ConversationSummaryMemory(\n",
          "    llm=ChatOpenAI(temperature=0), \n",
          "    memory_key='history'\n",
          ")"
        ],
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Step 4: Define the Conversational Agent\n",
          "\n",
          "Here, we create an instance of **ChatOpenAI** with a moderate `temperature` (i.e., creativity level). Then, we wrap it in a **ConversationChain** so the memory can be used automatically behind the scenes.\n",
          "\n",
          "Note: We’re only passing in `summary_memory` here—feel free to experiment by combining it with `vector_memory` or exploring advanced memory setups in LangChain."
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "# Initialize chatbot with advanced memory\n",
          "llm = ChatOpenAI(temperature=0.7)\n",
          "conversation_chain = ConversationChain(\n",
          "    llm=llm,\n",
          "    memory=summary_memory,\n",
          "    verbose=True\n",
          ")"
        ],
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Step 5: Chat with Memory\n",
          "\n",
          "Now, the fun part! We’ll prompt our agent with a series of messages. Notice how it remembers who you are and that you love space exploration.\n",
          "\n",
          "Feel free to add your own queries or ask the agent to recall details from earlier in the conversation. This is where you see memory in action!"
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "# Sample conversation using multiple memory sources\n",
          "print(conversation_chain.predict(input='Hello! What do you know about space?'))\n",
          "print(conversation_chain.predict(input='Can you remember my name is Alex and I love space exploration?'))\n",
          "print(conversation_chain.predict(input='What do you remember about me?'))\n",
          "print(conversation_chain.predict(input='Can you suggest a book about space exploration?'))"
        ],
        "execution_count": null,
        "outputs": []
      }
    ]
  }