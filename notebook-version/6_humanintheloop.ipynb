{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.types import Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, set your OPENAI_API_KEY here for demonstration, not recommended in production:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleState(TypedDict):\n",
    "    \"\"\"State with a possible user_input key, plus an output for demonstration.\"\"\"\n",
    "    user_input: str\n",
    "    output: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Node Functions\n",
    "\n",
    "human_input_node: capturing user input\n",
    "\n",
    "process_node: processes the user input by either manipulating it or utilizing a language model (e.g., GPT-4o) to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_input_node(state: ExampleState) -> ExampleState:\n",
    "    \"\"\"\n",
    "    In a real setting, you'd do `interrupt()` here or \n",
    "    handle user input in some special way. \n",
    "    For demonstration, we'll just read from state['user_input'] \n",
    "    as if the user typed something in.\n",
    "    \"\"\"\n",
    "    print(f\"[human_input_node] Received input: {state['user_input']}\")\n",
    "    return {\n",
    "        \"user_input\": state[\"user_input\"],\n",
    "        \"output\": state[\"output\"]\n",
    "    }\n",
    "\n",
    "def process_node(state: ExampleState) -> ExampleState:\n",
    "    \"\"\"\n",
    "    A trivial node that calls GPT-4o or just manipulates the input.\n",
    "    We'll just say we processed the user_input in some way.\n",
    "    \"\"\"\n",
    "    # Example: calling GPT-4o (this is optional)\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    response = llm.invoke([{\"role\": \"user\", \"content\": f\"Received input: {state['user_input']}\"}])\n",
    "    new_output = response.content\n",
    "\n",
    "    return {\n",
    "        \"user_input\": state[\"user_input\"],\n",
    "        \"output\": f\"[Processed] => {new_output}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(ExampleState)\n",
    "builder.add_node(\"human_input\", human_input_node)\n",
    "builder.add_node(\"process_node\", process_node)\n",
    "\n",
    "builder.add_edge(START, \"human_input\")\n",
    "builder.add_edge(\"human_input\", \"process_node\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with a breakpoint BEFORE human_input.\n",
    "# This means the graph halts execution right before we run 'human_input' \n",
    "# so that we can update the state with a \"user_input\" from an external source.\n",
    "graph = builder.compile(\n",
    "    checkpointer=memory,\n",
    "    interrupt_before=[\"human_input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Graph Step-by-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example():\n",
    "    # We'll define some minimal initial state with an empty user_input\n",
    "    initial_state = {\n",
    "        \"user_input\": \"\",\n",
    "        \"output\": \"\"\n",
    "    }\n",
    "    config = {\"configurable\": {\"thread_id\": \"example_thread\"}}\n",
    "\n",
    "    # 1) Stream the graph up to the breakpoint\n",
    "    print(\"=== Stream until the breakpoint ===\")\n",
    "    for event in graph.stream(initial_state, config, stream_mode=\"values\"):\n",
    "        print(event)\n",
    "\n",
    "    # At this point, the graph has paused BEFORE 'human_input' runs.\n",
    "    # So we can 'inject' the real user input or changes:\n",
    "    user_input_text = \"Hello supply chain advanced team!\"\n",
    "    print(f\"Injecting user_input: '{user_input_text}'\")\n",
    "\n",
    "    # 2) Use update_state(...) to inject the user_input \n",
    "    #    into the node 'human_input' specifically:\n",
    "    graph.update_state(\n",
    "        config,\n",
    "        {\"user_input\": user_input_text},\n",
    "        as_node=\"human_input\"\n",
    "    )\n",
    "\n",
    "    # 3) Continue execution from that checkpoint \n",
    "    #    so it runs 'human_input' next\n",
    "    print(\"=== Resuming Execution ===\")\n",
    "    for event in graph.stream(None, config, stream_mode=\"values\"):\n",
    "        print(event)\n",
    "\n",
    "    # 4) Finally, let's see the final state\n",
    "    final_state = graph.get_state(config)\n",
    "    print(\"=== Final State ===\")\n",
    "    print(final_state.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_example()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
